<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
    | ECE, Virginia Tech | Fall 2020: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <link href="css/custom_styles.css" rel="stylesheet">
  <style>
    body {
      padding-top: 60px;
      /* 60px to make the container go all the way to the bottom of the topbar */
    }

    .vis {
      color: #3366CC;
    }

    .data {
      color: #FF9900;
    }
  </style>

  <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
  <div class="container">
    <div class="page-header">

      <!-- Title and Name -->
      <h1>Final Report</h1>
      <h1>Visual Search Based Recommendation System</h1>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>Syeda Arzoo Irshad, Noah Meine, Jared
          Veerhoff</strong></span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Fall 2020 ECE 4554/5554 Computer Vision: Course
        Project</span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
      <hr>

      <!-- T-SNE Embedding GIF -->
      <div style="text-align: center; display: flex; margin: 0;">
        <figure class="figure" style="float: left;">
          <img style="height: 400px; width: 100%;" alt="" src="example_hoodies.png">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion
            </span>
          </figcaption>
        </figure>
        <figure class="figure" style="float: left; width: 100%;">
          <img style="height: 400px;" alt="" src="tsne_gif.gif">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              t-SNE Embeddings of the Fashion MNIST Dataset
            </span>
          </figcaption>
        </figure>
      </div>


      <!-- Goal -->
      <h3>Abstract</h3>

      Many current visual recommendation systems do not allow a user to visualize the difference or similarity
      between
      images. In this project, we implement our own visual search based recommendation
      system. This project will allow a user to visualize the similarity between their input image and the images in the database in
      low-dimensional space. Our project is tailored to work with fashion items (clothes, brands, outfits), as there are many
      exciting applications for visual recommendation in the fashion/e-commerce domain.<br><br>

      We use a pre-trained model in image classification based on the <a href="https://arxiv.org/abs/1512.03385"
        target="_blank">ResNet18</a> model because of its strong performance in
      visual recognition. We then implement transfer learning to optimize its performance with fashion items by
      using the <a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" target="_blank">DeepFashion</a> and
      Fashion MNIST datasets. We implement our own algorithm to generate image feature vectors and use Spotify's <a href="https://github.com/spotify/annoy" target="_blank">Annoy</a> library to access similar images.
      <br><br>

      Additionally, we compare the above transfer learning based model with a
      <a href="https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf"
        target="_blank">Siamese neural network </a>
      trained using <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html"
        target="_blank">Triplet loss based approach </a>
      The input to the model will consist of a triplet of images – query image, positive image (closely related to
      the query image), and a negative image (different from the query image). The model is trained on the Fashion MNIST
      dataset to generate embeddings
      for an input image using the triplet loss based approach such that the distance between embeddings of similar images
      (query and positive) will be lesser than that of dissimilar images (query and negative image).<br>

      <br><br>

      Finally, we adopt the model with the better performance (in terms of speed and accuracy)
      to develop a visual search based recommender to provide the user with the top-N most recommended images,
      and perform a low-deminsional visualization of image similarity
      using <a href="https://lvdmaaten.github.io/tsne/" target="_blank">t-SNE</a>.

      <br><br>

      We have deployed our project as a self-contained application using <a href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb"
      target="_blank">Google Colab</a>. Our application achieves a high level of
      qualitative accuracy for image recommendations.  After applying transfer learning, our model achieves a top-5 accuracy of 91%
      on the DeepFashion dataset (see experimental results). Our application is scalable and allows users to query similar images quickly.
      We aim to extend our project into a stand-alone web application.

      <br><br>
      <h3>Introduction</h3>
      Text based queries often don’t yield accurate results. For instance,
      one comes across an object of interest (a dress that someone’s wearing or a
      show piece in a store that’s not for sale). A text based search for this object
      on the internet can yield closely related items but may not retrieve the exact item.
      Visual search based systems can effectively solve this problem by allowing the user
      to query an image instead of a text and retrieve more accurate results.
      This project aims to develop a visual search based recommender system that
      will allow the user to input a query image and generate a list of top-N most
      similar images from our database.
        <br><br>
      We will utilize the ResNet18 model that was pre-trained on the vast ImageNet dataset
      and apply transfer learning methods to retrain the network
      for a specific fashion items based dataset that suits our projects goals.
      We will also experiment with a Siamese neural network trained with Triplet loss on the fashion MNIST dataset
      and compare results with the transfer learning based approach.
      This project finds use in applications like Pinterest where users can capture images on their
      phone cameras and upload it on the app to find similar images.

      <br><br>



      <!-- figure -->
      <h3>Approach - todo</h3>
      The flowchart below demonstrates the approach we have followed to accomplish our project goals.
      <br><br>
      <!-- Main Illustrative Figure -->
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="updated_flowchart.png">
      </div>

      <br><br>
      The technical approach we followed began with using a pretrained model (ResNet18) on the ImageNet dataset and adapting it to the fashion
      domain using transfer learning. To do this, we utilized multiple tools and services. We used python and relevant data processing libraries
      (numpy, pandas, matplotlib, etc.) to download and organize the DeepFashion dataset. Then, we used <a href="https://docs.fast.ai/" target="_blank">FastAI</a>
      and <a href="https://pytorch.org/" target="_blank">PyTorch</a> libraries to simiplify
      deep learning tasks such as generating image embeddings, performing transfer learning, and saving feature vectors from our model easily.
      In addition, we used cloud computing services such as Google Colab for access to optimized GPU hardware and for hosting our application.
      In order to manage the DeepFashion dataset, we utilized reference material from
      <a href="https://towardsdatascience.com/building-a-personalized-real-time-fashion-collection-recommender-22dc90c150cb" target="_blank">
        Building a Personalized Real-Time Fashion Collection Recommender.
      </a><br><br>
      After applying transfer learning, we evaluated the performance and cost of different models. <br><br>

      <strong>## ADD relevant info here about model evaluation steps and baseline models</strong><br><br>

      We then selected the optimal model and produced a library of feature vectors for each image in our dataset. We compiled these feature vectors
      in a database of recommended images to choose from, and stored relevant image data, embeddigns, and labels. Our next step was to allow the user
      to obtain recommended images from an input image outside of our dataset. Given the size of the DeepFashion data set and the hardware
      constraints of our system, we leveraged Spotify’s open-source Annoy (Approximate Nearest-Neighbor) library. Using Annoy, we created a query
      tree full of feature vectors from our model. Then we used our model to generate a feature vector for a user input image, and Annoy to
      obtain the top-N most similar images. The last step is to utilize the python sklearn libraries’ popular t-SNE framework to generate a
      visualization of image similarity in low-dimensional space. We return the user a list of similar images and the t-SNE plot. <br><br>

      <strong>## ADD relevant info here about any other steps you took in your approaches</strong>


      <br><br>

      <strong>Obstacles- ADD ANY OBSTACLES HERE</strong><br><br>
      One obstacle we faced at the beggining of the project was trying to manage the DeepFashion dataset given its large size and computational requirements.
      To overcome this obstacle, we utilized the refrence material listed under the approach. Another obstacle we encountered while attempting to implement transfer learning
      was trying to speed up the training process in order to perform more experiments. We overcame this obstacle with the help of
      Google Colab, which allows free access to optimized GPU hardware.


      <br><br>
      <!-- Results -->
      <h3>Experiments and Results</h3>
      <strong>ResNet18 and DeepFashion</strong><br><br>
        The DeepFashion dataset contains over 200K images with descriptive labels across 46 different categories. This dataset is well known
        for complex image classification and similarity problems. The images are different sizes, making it a good fit for our intented application.
        The set of images we used to train our model consisted of 209,222 training images, 40,000 validation images, and 40,000 test images.
        Next, we used this dataset to conduct transfer learning on the pretrained model.
        In order to apply transfer learning, we first determined the learning rate
        we should limit our model to. Below is a plot of our learning rate findings. Tweaking the learning rate value led to a change in the total time
        it took to train our model. <br><br>

        <div style="text-align: center; margin-top: 20px;">
          <figure class="figure">
            <img style="height: 200px;" alt="" src="learning_rate_resnet18.png">
            <figcaption class="figure-caption">
              <span style="font-size: 12px; line-height: 1.5em;">
                Learning Rate Suggestion
              </span>
            </figcaption>
          </figure>
        </div> <br><br>

        Using this learning rate, we applied transfer learning. The metrics we used to evaluate the model were accuracy, top-1 accuracy, and top-5 accuracy. We included top-N accuracy because it is relevant
        to our desired use case. We want to provide the user with a collection of similar images, not a single prediction.
        The complete summary of the transfer learning process is shown below. <br><br>
        <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
          <samp>
            ----------------------------------------------------------------------------------------------------<br>
            epoch&nbsp;&nbsp;&nbsp;&nbsp;train_loss&nbsp;&nbsp;&nbsp;&nbsp;valid_loss&nbsp;&nbsp;&nbsp;&nbsp;accuracy&nbsp;&nbsp;&nbsp;&nbsp;top_1_accuracy&nbsp;&nbsp;&nbsp;&nbsp;top_5_accuracy&nbsp;&nbsp;&nbsp;&nbsp;time<br>
            0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.639774&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.476857&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.566800&nbsp;&nbsp;&nbsp;&nbsp;0.566800&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.885500&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:57<br>
            1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.583464&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.486946&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.565150&nbsp;&nbsp;&nbsp;&nbsp;0.565150&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.876175&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:55<br>
            2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.583681&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.493149&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.559150&nbsp;&nbsp;&nbsp;&nbsp;0.559150&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.874550&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:00<br>
            3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.532783&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.467412&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.576425&nbsp;&nbsp;&nbsp;&nbsp;0.576425&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.879400&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:13<br>
            4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.522732&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.369081&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.596525&nbsp;&nbsp;&nbsp;&nbsp;0.596525&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.894675&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:06<br>
            5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.461197&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.363835&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.597000&nbsp;&nbsp;&nbsp;&nbsp;0.597000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.895925&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:55<br>
            6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.406832&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.316558&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.610000&nbsp;&nbsp;&nbsp;&nbsp;0.610000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.903850&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:54<br>
            7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.371371&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.274097&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.623450&nbsp;&nbsp;&nbsp;&nbsp;0.623450&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.911050&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:05<br>
            8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.360969&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.248707&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.631025&nbsp;&nbsp;&nbsp;&nbsp;0.631025&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.913025&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:02<br>
            9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.354846&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.246133&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.632725&nbsp;&nbsp;&nbsp;&nbsp;0.632725&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.913025&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:59<br>
            ----------------------------------------------------------------------------------------------------<br>
          </samp>
        </div> <br><br>

        Finally, we evaluated our model in a variety of ways. First, we observed the confusion matrix generated from our model, and then ploted some of the top losses from our model.
        The confusion matrix and an array of the top losses can be seen below. Both of these images, along with the metrics from training indicate a reasonable level of performance from our model. <br><br>

        <div style="text-align: center; display: flex; margin: 0;">
          <figure class="figure" style="float: left;">
            <img style="height: 400px; width: 100%;" alt="" src="confusion_matrix_resnet18.png">
            <figcaption class="figure-caption">
              <span style="font-size: 12px; line-height: 1.5em;">
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Confusion Matrix (zoom in)
              </span>
            </figcaption>
          </figure>
          <figure class="figure" style="float: left; width: 100%;">
            <img style="height: 400px;" alt="" src="top_losses_resnet18.png">
            <figcaption class="figure-caption">
              <span style="font-size: 12px; line-height: 1.5em;">
                Top Losses (zoom in)
              </span>
            </figcaption>
          </figure>
        </div> <br><br>


        <br><br>

      <strong>VGG16 and Fashion-MNIST</strong><br><br>
      In order to perform transfer learning on the VGG16 model, we used the Fashion MNIST dataset. This dataset is well known for fashion
      related image classification and allows us to produce feature vectors well alongside the VGG architecture. The dataset contains
      a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image that contains a label from 10 possible classes.
      The training and test labels are from one of the following classes: <br><br>
      <ul>
        <li>0 T-shirt/top</li>
        <li>1 Trouser</li>
        <li>2 Pullover</li>
        <li>3 Dress</li>
        <li>4 Coat</li>
        <li>5 Sandal</li>
        <li>6 Shirt</li>
        <li>7 Sneaker</li>
        <li>8 Bag</li>
        <li>9 Ankle boot</li>
      </ul>
      <br>
      <strong>Transfer Learning</strong><br><br>
      Next, we adapted the dataset to support the base VGG16 model by adding an additional two color channels and scaling the images to 48x48 pixels using openCV
      methods. We then defined our base VGG16 model using pretrained weights and removed the top layer used for classification. In order to adapt the model to
      the Fashion MNIST dataset, we added several layers to flatten the last VGG16 output layer and classify the images into 10 classes. Below is a summary of the model
      that was created. <br><br>

      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
        <samp>
          _________________________________________________________________<br>
          Layer (type)        Output Shape        Param #   <br>
          =================================================================<br>
          vgg16 (Functional)           (None, 1, 1, 512)         14714688  <br>
          _________________________________________________________________ <br>
          flatten_1 (Flatten)          (None, 512)               0          <br>
          _________________________________________________________________ <br>
          fc1 (Dense)                  (None, 1024)              525312     <br>
          _________________________________________________________________ <br>
          predictions (Dense)          (None, 10)                10250      <br>
          ================================================================= <br>
          Total params: 15,250,250 <br>
          Trainable params: 7,614,986 <br>
          Non-trainable params: 7,635,264 <br>
        </samp>
      </div>
      <br><br>

      The next step in our approach was applying transfer learning. We used categorical (one hot) encoding for our
      training, validation, and test labels. In addition, we used a train/test split with a test size of 0.2. Here is an excerpt of our training
      script that uses the sklearn train_test_split() function. <br><br>
      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
        <samp>
          x_train, x_validate, y_train_one_hot, y_validate_one_hot = train_test_split(<br>
          &nbsp;&nbsp;&nbsp;&nbsp; x_train, y_train_one_hot, test_size=0.2, random_state=13,<br>
        )
        </samp>
      </div> <br>

      The metrics that we used to evaluate our model during the transfer learning procedure were categorical accuracy and categorical crossentropy for loss.
      We gathered data about the transfer learning process using tensorboard, and performed testing on the model post-transfer learning as well. Below is a complete summary of
      the transfer learning procedure, including plots generated using tensorboard. <br><br>

      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px; font-size: 12px;">
        <samp>
        Epoch 1/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.4312 - categorical_accuracy: 0.8467 - val_loss: 0.3378 - val_categorical_accuracy: 0.8791 <br>
        Epoch 2/12 <br>
        375/375 [==============================] - 23s 61ms/step - loss: 0.2597 - categorical_accuracy: 0.9059 - val_loss: 0.2599 - val_categorical_accuracy: 0.9071 <br>
        Epoch 3/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.2212 - categorical_accuracy: 0.9183 - val_loss: 0.2496 - val_categorical_accuracy: 0.9113 <br>
        Epoch 4/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.1972 - categorical_accuracy: 0.9269 - val_loss: 0.2621 - val_categorical_accuracy: 0.9104 <br>
        Epoch 5/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.1799 - categorical_accuracy: 0.9353 - val_loss: 0.2461 - val_categorical_accuracy: 0.9164 <br>
        Epoch 6/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.1655 - categorical_accuracy: 0.9399 - val_loss: 0.2256 - val_categorical_accuracy: 0.9213 <br>
        Epoch 7/12 <br>
        375/375 [==============================] - 24s 63ms/step - loss: 0.1519 - categorical_accuracy: 0.9447 - val_loss: 0.2367 - val_categorical_accuracy: 0.9214 <br>
        Epoch 8/12 <br>
        375/375 [==============================] - 23s 63ms/step - loss: 0.1391 - categorical_accuracy: 0.9493 - val_loss: 0.2597 - val_categorical_accuracy: 0.9179 <br>
        Epoch 9/12 <br>
        375/375 [==============================] - 24s 63ms/step - loss: 0.1280 - categorical_accuracy: 0.9537 - val_loss: 0.2467 - val_categorical_accuracy: 0.9189 <br>
        Epoch 10/12 <br>
        375/375 [==============================] - 24s 63ms/step - loss: 0.1201 - categorical_accuracy: 0.9561 - val_loss: 0.2632 - val_categorical_accuracy: 0.9241 <br>
        Epoch 11/12 <br>
        375/375 [==============================] - 23s 63ms/step - loss: 0.1139 - categorical_accuracy: 0.9591 - val_loss: 0.3109 - val_categorical_accuracy: 0.9132 <br>
        Epoch 12/12 <br>
        375/375 [==============================] - 23s 63ms/step - loss: 0.1070 - categorical_accuracy: 0.9618 - val_loss: 0.2575 - val_categorical_accuracy: 0.9220 <br>
        </samp>
      </div> <br>

      <div style="text-align: center; margin-top: 20px;">
        <figure class="figure">
          <img style="height: 200px;" alt="" src="epoch_categorical_accuracy_final.svg">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              Categorical Accuracy vs. Number of Epochs
            </span>
          </figcaption>
        </figure>
        <figure class="figure">
          <img style="height: 200px;" alt="" src="epoch_loss_final.svg">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              Categorical Crossentropy vs. Number of Epochs
            </span>
          </figcaption>
        </figure>
      </div> <br><br>

      Final test accuracy and loss. <br><br>
      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
        <samp>
          Test loss:  0.26292872428894043 <br>
          Test accuracy:  0.9182000160217285
        </samp>
      </div> <br>

      <strong>Deep Ranking - UPDATE/REMOVE AS NECESSARY</strong><br><br>
        In the next stage of the project we will implement another deep learning model
        for image similarity that is based on the deep ranking architecture. For this project,
        we will implement a simpler version of the triplet sampling technique to generate the
        triplets. Instead of an online sampling technique, we will use uniform sampling to
        generate the triplets or utilize an existing dataset of triplets
        <a href="https://sites.google.com/site/imagesimilaritydata/download"
        target="_blank">triplet.txt</a>
        and augment it if necessary.
        <br><br>
        For the convnet segment, we will replace the AlexNet
        with VGG16 (similar to the Visnet architecture) in Keras and use the concatenate
        function in Keras to combine it with the two shallow convolutional layers.
        The final Ranking layer will compute the Hinge loss or a Triplet loss to train the
        network based on relative similarity between the triplet images.
        <br><br>

        <strong>Discussion of Models, Datasets, and Results</strong><br><br>

        For our application, the ResNet18 model and DeepFashion dataset were selected for use in the final application. This is because
        the DeepFashion dataset is much more representative of a user image query than Fashion-MNIST, and produces a high level of accuracy. ResNet18 was selected
        over VGG16 because it was more efficient to train and use, while providing better results for our intented application.

      <!-- Results -->
      <h3>Qualitative Results</h3>
      <strong>Visual Recommendations</strong><br><br>
          Here is an example of two visual recommendations. In each recommendation, the user input image is listed on the left and the top 9 most similar images from the dataset
          are displayed next to it. Also find a t-SNE plot of the corresponding images below. <br><br>
          <!-- T-SNE Embedding GIF -->
      <div style="text-align: center; width:100%; height: 600px; position: relative;">
        <figure class="figure" style="position: absolute; top:0; left: -100px; width: 50%">
          <img style="height: 600px;" alt="" src="example_hoodies.png">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion
            </span>
          </figcaption>
        </figure>
        <figure class="figure" style="position: absolute; top:0; right: -100px; width: 50%">
          <img style="height: 600px;" alt="" src="lakers.JPG">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion
            </span>
          </figcaption>
        </figure>
      </div>
      <br><br><br>
      <div style="text-align: center; width:100%; height: 400px; position: relative;">
        <figure class="figure" style="position: absolute; top:-40px; left: -100px; width: 50%">
          <img alt="" src="hoodie-tsne.JPG">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t-SNE Hoodie Projections
            </span>
          </figcaption>
        </figure>
        <figure class="figure" style="position: absolute; top:0; right: -100px; width: 50%">
          <img alt="" src="lakers-tsne.JPG">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t-SNE Jersey Projections
            </span>
          </figcaption>
        </figure>
      </div>
      <br><br><br>
      <!-- Results -->
      <h3>Conclusion and Future Work</h3>
      In the final stage of the project, we will test the above models on different query
      images and generate the top-N most similar items using distance metrics like L1, L2
      norm or use <i>k</i>-NN. The performace of We will compare the performance of the above architectures and
      use the best performing model in our visual search based recommender system where
      the user can query any input image in the fashion domain to obtain top-N most similar
      items from the database.
      <br><br>

      <!-- Results -->
      <h3>References</h3>
      Fashion MNIST
      <br>
      <a href="https://www.kaggle.com/zalando-research/fashionmnist">https://www.kaggle.com/zalando-research/fashionmnist</a>

      <br><br>
      A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning | by Dipanjan (DJ) Sarkar | Towards Data Science
      <br>
      <a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">
               https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a</a>
      <br><br>
      Transfer Learning with Convolutional Neural Networks in PyTorch | by Will Koehrsen | Towards Data Science
      <br>
      <a href="https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce">
               https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce</a>

      <br><br>
      Embeddings Visualization with Fashion MNIST
      <br>
      <a href="https://github.com/markjay4k/Fashion-MNIST-with-Keras/blob/master/pt3%20-%20FMINST%20Embeddings.ipynb">
        https://github.com/markjay4k/Fashion-MNIST-with-Keras/blob/master/pt3%20-%20FMINST%20Embeddings.ipynb</a>

      <br><br>
      Visualizing Data using the Embedding Projector in TensorBoard
      <br>
      <a href="https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin">https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin</a>

      <br><br>
      Learning Fine-grained Image Similarity with Deep Ranking
      <br>
      <a href="https://arxiv.org/pdf/1404.4661.pdf">https://arxiv.org/pdf/1404.4661.pdf</a>

      <br><br>
      Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce
      <br>
      <a href="https://arxiv.org/pdf/1703.02344.pdf">https://arxiv.org/pdf/1703.02344.pdf</a>

      <br><br>
      Building a Personalized Real-Time Fashion Collection Recommender
      <br>
      <a href="https://towardsdatascience.com/building-a-personalized-real-time-fashion-collection-recommender-22dc90c150cb">https://towardsdatascience.com/building-a-personalized-real-time-fashion-collection-recommender-22dc90c150cb</a>


      <br><br>


      <hr>
      <footer>
        <p>© Arzoo Irshad, Noah Meine, Jared Veerhoff</p>
      </footer>
    </div>
  </div>

  <br><br>

</body>

</html>
