<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
    | ECE, Virginia Tech | Fall 2020: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <link href="css/custom_styles.css" rel="stylesheet">
  <style>
    body {
      padding-top: 60px;
      /* 60px to make the container go all the way to the bottom of the topbar */
    }

    .vis {
      color: #3366CC;
    }

    .data {
      color: #FF9900;
    }
  </style>

  <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
  <div class="container">
    <div class="page-header">

      <!-- Title and Name -->
      <h1>Final Report</h1>
      <h1>Visual Search Based Recommendation System</h1>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>Syeda Arzoo Irshad, Noah Meine, Jared
          Veerhoff</strong></span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Fall 2020 ECE 4554/5554 Computer Vision: Course
        Project</span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
      <hr>

      <!-- T-SNE Embedding GIF -->
      <div style="text-align: center; display: flex; margin: 0;">
        <figure class="figure" style="float: left;">
          <img style="height: 400px; width: 100%;" alt="" src="example_hoodies.png">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion
            </span>
          </figcaption>
        </figure>
        <figure class="figure" style="float: left; width: 100%;">
          <img style="height: 400px;" alt="" src="tsne_gif.gif">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              t-SNE Embeddings of the Fashion MNIST Dataset
            </span>
          </figcaption>
        </figure>
      </div>


      <!-- Goal -->
      <h3>Abstract</h3>

      Many current visual recommendation systems do not allow a user to visualize the difference or similarity
      between
      images. In this project, we implement our own visual search based recommendation
      system. This project will allow a user to visualize the similarity between their input image and the images in the database in
      low-dimensional space. Our project is tailored to work with fashion items (clothes, brands, outfits), as there are many
      exciting applications for visual recommendation in the fashion/e-commerce domain.<br><br>

      We use a pre-trained model in image classification based on the <a href="https://arxiv.org/abs/1512.03385"
        target="_blank">ResNet18</a> model because of its strong performance in
      visual recognition. We then implement transfer learning to optimize its performance with fashion items by
      using the <a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" target="_blank">DeepFashion</a> and
      Fashion MNIST datasets. We implement our own algorithm to generate image feature vectors and use
      Spotify's <a href="https://github.com/spotify/annoy" target="_blank">Annoy</a> library to access similar images using approximate nearest-neighbor.
      <br><br>

      Additionally, we compare the above transfer learning based model with a
      <a href="https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf"
        target="_blank">Siamese neural network </a>
      trained using <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html"
        target="_blank">Triplet loss based approach </a>
      The input to the model will consist of a triplet of images – query image, positive image , and a negative image . The model is trained on the Fashion MNIST
      dataset to generate embeddings
      for an input image using the triplet loss based approach such that the distance between embeddings of similar images
      (query and positive) will be lesser than that of dissimilar images (query and negative image).

      <br><br>

      Finally, we adopt the model with the better performance
      to develop a visual search based recommender to provide the user with the top-N most recommended images,
      and perform a low-dimensional visualization of image similarity
      using <a href="https://lvdmaaten.github.io/tsne/" target="_blank">t-SNE</a>.

      <br><br>

      We have deployed our project as a self-contained application using <a href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb"
      target="_blank">Google Colab</a>. Our application achieves a high level of
      qualitative accuracy for image recommendations.  After applying transfer learning, our model achieves a top-5 accuracy of 91%
      on the DeepFashion dataset (see experimental results). Our application is scalable and allows users to query similar images quickly.

      <br><br>
      <h3>Introduction</h3>
      Text based queries often do not yield accurate results. For instance,
      an individual comes across a dress someone else is wearing or a show piece
      in a store that is not for sale. A text-based search for this object on the internet can yield
      closely related items but may not retrieve the exact item, and results will vary based on an
      individual's choice of words. Visual-based search systems can solve this problem by allowing the
      user to query an image instead of text and retrieve more accurate and consistent results. This project
      aims to develop a visual search based recommender system that will allow the user to input a query
      image and generate a list of top-N most similar images from our database.

        <br><br>
      This project can find use in applications such as Pinterest, where users can capture images on their
      phone cameras and upload it on the app to find similar images. An online clothing store could implement a
      visual reccomendation system to help a customer find something they might only have a picture of.

      <br><br>

      The area of convolutional neural networks (CNN) is a rapidly changing field with increasingly more applications each year. One of the challenges
      of working with CNNs is the high demand for resources, especially when working with images. Low-dimensional space in this report refers to the reduction of
      data in high-dimensional space down to something easily understandable. In this case we reduce from 512 dimensions down to three, which allows for us to provide
      the end user with a simple three-dimensional plot using <a href="https://plotly.com/python/">Plotly</a>.
      <br><br>


      <!-- figure -->
      <h3>Approach </h3>
      The flowchart below demonstrates the approach we have followed to accomplish our project goals.
      <br><br>
      <!-- Main Illustrative Figure -->
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="updated_flowchart.png">
      </div>

      <br><br>
      The technical approach we followed began with using a pretrained model (ResNet18) on the ImageNet dataset and adapting it to the fashion
      domain using transfer learning. To do this, we utilized multiple tools and services. We used python and relevant data processing libraries
      (numpy, pandas, matplotlib, etc.) to download and organize the DeepFashion dataset. Then, we used <a href="https://docs.fast.ai/" target="_blank">FastAI</a>
      and <a href="https://pytorch.org/" target="_blank">PyTorch</a> libraries to simplify
      deep learning tasks such as generating image embeddings, performing transfer learning, and saving feature vectors from our model easily.
      In addition, we used cloud computing services such as Google Colab for access to optimized GPU hardware and for hosting our application.
      In order to manage the DeepFashion dataset, we utilized reference material from
      <a href="https://towardsdatascience.com/building-a-personalized-real-time-fashion-collection-recommender-22dc90c150cb" target="_blank">
        Building a Personalized Real-Time Fashion Collection Recommender.
      </a><br><br>
      After applying transfer learning, we evaluated the performance and cost of different models.
      <br><br>
      The Siamese Neural Network (SNN) was first proposed in 1993 to verify signatures written on a pen-input tablet. The architecture consists of two or more ‘identical’ subnetworks, i.e., they possess the same configuration and share the same parameters and weights. SNN learns to find the similarity between the inputs by comparing their corresponding feature vectors generated from the subnetworks. Siamese neural networks are also known to  be more robust to class imbalance and can effectively capture the semantic similarity between the images in their respective embeddings. Conventional neural networks need to be retrained when a new class is added to or removed from the database. Conversely, SNNs can classify new classes of data without needing to be retrained as they learn a ‘similarity function’ that determines if two images belong to the same class.

      <br><br>

      The Trip Loss technique was introduced in the Facenet paper in 2015. In the triplet loss function, an anchor image is compared to a positive input and a negative input image. The distance between the anchor and positive input is minimized while the
      distance between the anchor and negative input is maximized.

      The loss function is defined as below –

      <br><br>
      <div style="text-align: center;">
        <img style="height: 100px;" alt="" src="TripletLoss.png">
      </div>
      <br><br>
      In the above equation, alpha is the margin that specifies how further apart must the similar and dissimilar pairs of the triplets be in the distribution space. The margin hyperparameter helps distinguish the two images better. f(A), f(N), and f(P) are the feature embeddings of the anchor, positive and negative images.

      <br><br>
      During the training process, an images triplet is fed into identical models. Since the weights of the models are identical, we use a single model and feed the three images sequentially. Following this, the loss is calculated as above such that the distance between anchor and positive image is minimized and that of anchor and negative images is maximized. Using the loss function, we calculate the gradients and update the weights and biases of the network. In this experiment, we have taken an anchor image and randomly sampled positive and negative images to compute the loss function and update the gradients.
      <br><br>
      The embeddings generated for each of the images serve as their latent feature representations. A comparison of these embeddings can reveal if two images belong to the same class or not. The dimension of the embeddings is another hyperparameter of our network that can be tuned to optimize the system performance.
      SNNs work well in scenarios where the classes change too frequently to be constantly retrained.
      <br><br>
      We then selected the optimal model and produced a library of feature vectors for each image in our dataset. We compiled these feature vectors
      in a database of recommended images to choose from, and stored relevant image data, embeddings, and labels. Our next step was to allow the user
      to obtain recommended images from an input image outside of our dataset. Given the size of the DeepFashion data set and the hardware
      constraints of our system, we leveraged Spotify’s open-source Annoy (Approximate Nearest-Neighbor) library. Using Annoy, we created a query
      tree full of feature vectors from our model. Then we used our model to generate a feature vector for a user input image and Annoy to
      obtain the top-N most similar images. The last step is to utilize the python sklearn libraries’ popular t-SNE framework to generate a
      visualization of image similarity in low-dimensional space. We return the user a list of similar images and the t-SNE plot. <br><br>

      With the system now working, the model was replaced with its pre-transfer learning counterpart.
      This was done to ensure there was noticeable improvement in qualitative performance as a result of the transfer learning.
      Identical input images were provided to the two systems and the resulting top-N similar images were compared.


      <br><br>

      <strong>Obstacles</strong><br><br>
      One obstacle we faced at the beginning of the project was trying to manage the DeepFashion dataset given its large size and computational requirements.
      To overcome this obstacle, we utilized the reference material listed under the approach. Another obstacle we encountered while attempting to implement transfer learning
      was trying to speed up the training process in order to perform more experiments. We overcame this obstacle with the help of
      Google Colab, which allows free access to optimized GPU hardware.

      <!-- Judgement calls would go here -->


      <br><br>
      <!-- Results -->
      <h3>Experiments and Results</h3>
      <strong>ResNet18 and DeepFashion</strong><br><br>
        The DeepFashion dataset contains over 200K images with descriptive labels across 46 different categories. This dataset is well known
        for complex image classification and similarity problems. The images are different sizes, making it a good fit for our intended application.
        The set of images we used to train our model consisted of 209,222 training images, 40,000 validation images, and 40,000 test images.
        Next, we used this dataset to conduct transfer learning on the pretrained model.
        In order to apply transfer learning, we first determined the learning rate
        we should limit our model to. Below is a plot of our learning rate findings. Tweaking the learning rate value led to a change in the total time
        it took to train our model. <br><br>

        <div style="text-align: center; margin-top: 20px;">
          <figure class="figure">
            <img style="height: 200px;" alt="" src="learning_rate_resnet18.png">
            <figcaption class="figure-caption">
              <span style="font-size: 12px; line-height: 1.5em;">
                Learning Rate Suggestion
              </span>
            </figcaption>
          </figure>
        </div> <br><br>

        Using this learning rate, we applied transfer learning. The metrics we used to evaluate the model were accuracy, top-1 accuracy, and top-5 accuracy. We included top-N accuracy because it is relevant
        to our desired use case. We want to provide the user with a collection of similar images, not a single prediction.
        The complete summary of the transfer learning process is shown below. <br><br>
        <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
          <samp>
            ----------------------------------------------------------------------------------------------------<br>
            epoch&nbsp;&nbsp;&nbsp;&nbsp;train_loss&nbsp;&nbsp;&nbsp;&nbsp;valid_loss&nbsp;&nbsp;&nbsp;&nbsp;accuracy&nbsp;&nbsp;&nbsp;&nbsp;top_1_accuracy&nbsp;&nbsp;&nbsp;&nbsp;top_5_accuracy&nbsp;&nbsp;&nbsp;&nbsp;time<br>
            0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.639774&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.476857&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.566800&nbsp;&nbsp;&nbsp;&nbsp;0.566800&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.885500&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:57<br>
            1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.583464&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.486946&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.565150&nbsp;&nbsp;&nbsp;&nbsp;0.565150&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.876175&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:55<br>
            2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.583681&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.493149&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.559150&nbsp;&nbsp;&nbsp;&nbsp;0.559150&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.874550&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:00<br>
            3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.532783&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.467412&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.576425&nbsp;&nbsp;&nbsp;&nbsp;0.576425&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.879400&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:13<br>
            4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.522732&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.369081&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.596525&nbsp;&nbsp;&nbsp;&nbsp;0.596525&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.894675&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:06<br>
            5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.461197&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.363835&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.597000&nbsp;&nbsp;&nbsp;&nbsp;0.597000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.895925&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:55<br>
            6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.406832&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.316558&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.610000&nbsp;&nbsp;&nbsp;&nbsp;0.610000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.903850&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:54<br>
            7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.371371&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.274097&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.623450&nbsp;&nbsp;&nbsp;&nbsp;0.623450&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.911050&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:05<br>
            8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.360969&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.248707&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.631025&nbsp;&nbsp;&nbsp;&nbsp;0.631025&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.913025&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17:02<br>
            9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.354846&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.246133&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.632725&nbsp;&nbsp;&nbsp;&nbsp;0.632725&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.913025&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16:59<br>
            ----------------------------------------------------------------------------------------------------<br>
          </samp>
        </div> <br><br>

        Finally, we evaluated our model in a variety of ways. First, we observed the confusion matrix generated from our model, and then plotted some of the top losses from our model.
        The confusion matrix and an array of the top losses can be seen below. Both images, along with the metrics from training indicate a reasonable level of performance from our model. <br><br>

        <div style="text-align: center; display: flex; margin: 0;">
          <figure class="figure" style="float: left;">
            <img style="height: 400px; width: 100%;" alt="" src="confusion_matrix_resnet18.png">
            <figcaption class="figure-caption">
              <span style="font-size: 12px; line-height: 1.5em;">
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Confusion Matrix (zoom in)
              </span>
            </figcaption>
          </figure>
          <figure class="figure" style="float: left; width: 100%;">
            <img style="height: 400px;" alt="" src="top_losses_resnet18.png">
            <figcaption class="figure-caption">
              <span style="font-size: 12px; line-height: 1.5em;">
                Top Losses (zoom in)
              </span>
            </figcaption>
          </figure>
        </div> <br><br>


        <br><br>

      <strong>VGG16 and Fashion-MNIST</strong><br><br>
      In order to perform transfer learning on the VGG16 model, we used the Fashion MNIST dataset. This dataset is well known for fashion
      related image classification and allows us to produce feature vectors well alongside the VGG architecture. The dataset contains
      a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image that contains a label from 10 possible classes.
      The training and test labels are from one of the following classes: <br><br>
      <ul>
        <li>0 T-shirt/top</li>
        <li>1 Trouser</li>
        <li>2 Pullover</li>
        <li>3 Dress</li>
        <li>4 Coat</li>
        <li>5 Sandal</li>
        <li>6 Shirt</li>
        <li>7 Sneaker</li>
        <li>8 Bag</li>
        <li>9 Ankle boot</li>
      </ul>
      <br>
      <strong>Transfer Learning</strong><br><br>
      Next, we adapted the dataset to support the base VGG16 model by adding an additional two color channels and scaling the images to 48x48 pixels using openCV
      methods. We then defined our base VGG16 model using pretrained weights and removed the top layer used for classification. In order to adapt the model to
      the Fashion MNIST dataset, we added several layers to flatten the last VGG16 output layer and classify the images into 10 classes. Below is a summary of the model
      that was created. <br><br>

      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
        <samp>
          _________________________________________________________________<br>
          Layer (type)        Output Shape        Param #   <br>
          =================================================================<br>
          vgg16 (Functional)           (None, 1, 1, 512)         14714688  <br>
          _________________________________________________________________ <br>
          flatten_1 (Flatten)          (None, 512)               0          <br>
          _________________________________________________________________ <br>
          fc1 (Dense)                  (None, 1024)              525312     <br>
          _________________________________________________________________ <br>
          predictions (Dense)          (None, 10)                10250      <br>
          ================================================================= <br>
          Total params: 15,250,250 <br>
          Trainable params: 7,614,986 <br>
          Non-trainable params: 7,635,264 <br>
        </samp>
      </div>
      <br><br>

      The next step in our approach was applying transfer learning. We used categorical (one hot) encoding for our
      training, validation, and test labels. In addition, we used a train/test split with a test size of 0.2. Here is an excerpt of our training
      script that uses the sklearn train_test_split() function. <br><br>
      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
        <samp>
          x_train, x_validate, y_train_one_hot, y_validate_one_hot = train_test_split(<br>
          &nbsp;&nbsp;&nbsp;&nbsp; x_train, y_train_one_hot, test_size=0.2, random_state=13,<br>
        )
        </samp>
      </div> <br>

      The metrics that we used to evaluate our model during the transfer learning procedure were categorical accuracy and categorical crossentropy for loss.
      We gathered data about the transfer learning process using tensorboard and performed testing on the model post-transfer learning as well. Below is a complete summary of
      the transfer learning procedure, including plots generated using tensorboard. <br><br>

      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px; font-size: 12px;">
        <samp>
        Epoch 1/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.4312 - categorical_accuracy: 0.8467 - val_loss: 0.3378 - val_categorical_accuracy: 0.8791 <br>
        Epoch 2/12 <br>
        375/375 [==============================] - 23s 61ms/step - loss: 0.2597 - categorical_accuracy: 0.9059 - val_loss: 0.2599 - val_categorical_accuracy: 0.9071 <br>
        Epoch 3/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.2212 - categorical_accuracy: 0.9183 - val_loss: 0.2496 - val_categorical_accuracy: 0.9113 <br>
        Epoch 4/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.1972 - categorical_accuracy: 0.9269 - val_loss: 0.2621 - val_categorical_accuracy: 0.9104 <br>
        Epoch 5/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.1799 - categorical_accuracy: 0.9353 - val_loss: 0.2461 - val_categorical_accuracy: 0.9164 <br>
        Epoch 6/12 <br>
        375/375 [==============================] - 23s 62ms/step - loss: 0.1655 - categorical_accuracy: 0.9399 - val_loss: 0.2256 - val_categorical_accuracy: 0.9213 <br>
        Epoch 7/12 <br>
        375/375 [==============================] - 24s 63ms/step - loss: 0.1519 - categorical_accuracy: 0.9447 - val_loss: 0.2367 - val_categorical_accuracy: 0.9214 <br>
        Epoch 8/12 <br>
        375/375 [==============================] - 23s 63ms/step - loss: 0.1391 - categorical_accuracy: 0.9493 - val_loss: 0.2597 - val_categorical_accuracy: 0.9179 <br>
        Epoch 9/12 <br>
        375/375 [==============================] - 24s 63ms/step - loss: 0.1280 - categorical_accuracy: 0.9537 - val_loss: 0.2467 - val_categorical_accuracy: 0.9189 <br>
        Epoch 10/12 <br>
        375/375 [==============================] - 24s 63ms/step - loss: 0.1201 - categorical_accuracy: 0.9561 - val_loss: 0.2632 - val_categorical_accuracy: 0.9241 <br>
        Epoch 11/12 <br>
        375/375 [==============================] - 23s 63ms/step - loss: 0.1139 - categorical_accuracy: 0.9591 - val_loss: 0.3109 - val_categorical_accuracy: 0.9132 <br>
        Epoch 12/12 <br>
        375/375 [==============================] - 23s 63ms/step - loss: 0.1070 - categorical_accuracy: 0.9618 - val_loss: 0.2575 - val_categorical_accuracy: 0.9220 <br>
        </samp>
      </div> <br>

      <div style="text-align: center; margin-top: 20px;">
        <figure class="figure">
          <img style="height: 200px;" alt="" src="epoch_categorical_accuracy_final.svg">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              Categorical Accuracy vs. Number of Epochs
            </span>
          </figcaption>
        </figure>
        <figure class="figure">
          <img style="height: 200px;" alt="" src="epoch_loss_final.svg">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              Categorical Crossentropy vs. Number of Epochs
            </span>
          </figcaption>
        </figure>
      </div> <br><br>

      Final test accuracy and loss. <br><br>
      <div style="background-color: #F5F5F5; border-radius: 12px; padding: 8px">
        <samp>
          Test loss:  0.26292872428894043 <br>
          Test accuracy:  0.9182000160217285
        </samp>
      </div> <br>

      <strong>Siamese Neural Network with Triplet Loss</strong><br><br>
      In this project, we’ve employed the FashionMNIST dataset to generate the triplet input images
      and train the Siamese neural network. There are two ways to generate triplets –
      <a href="https://medium.com/@enoshshr/triplet-loss-and-siamese-neural-networks-5d363fdeba9b"
      target="_blank">offline and
      online triplet mining.</a>
      In this experiment, we’ve used the offline approach, where the triplets are generated manually and then fit
      to the network.
      <br><br>
      Following are the model training results.
      <br><br>
      <img style="height: 150px;" alt="" src="FashionMNIST.png">
      <br><br>
      <img style="height: 150px;" alt="" src="Siamese.png">
      <br><br>
      We also observed the following trend of increase in accuracy on increasing the number of
      training training samples per class for generating the triplets.
      <img style="height: 150px;" alt="" src="100_training_samples.png">
      <img style="height: 150px;" alt="" src="200_training_samples.png">
      <img style="height: 150px;" alt="" src="500_Training_samples.png">
      <br><br>

        <strong>Discussion of Models, Datasets, and Results</strong><br><br>
        The Siamese network produced an accuracy of ~80% in comparison to the Transfer Learning based model (based on VGG16) which
        produced accuracy of ~92% on the FashionMNIST. Considering its superior performance and the potential computationally
        expensive procedure involved in generating triplets on the DeepFashion dataset, the Transfer Learning based approach was
        selected for the subsequent stage of building the visual search system. <br><br>

        When evaluating the pre-trained ResNet18 model without transfer learning as a feature vector generator, it is a clear failure.
        The top-N similar images do not bear much resemblance to the provided input image. Interestingly, the top-N
        similar images do seem to resemble each other somewhat. It can be concluded that performing transfer learning on the
        ResNet18 model resulted in a vast improvement over a naïve approach with the pre-trained model. <br><br>

        For our application, the ResNet18 model and DeepFashion dataset were selected for use in the final application. This is because
        the DeepFashion dataset is much more representative of a user image query than Fashion-MNIST, and produces a high level of accuracy. ResNet18 was selected
        over VGG16 because it was more efficient to train and use, while providing better results for our intented application.

      <!-- Results -->
      <h3>Qualitative Results</h3>
      <strong>Visual Recommendations</strong><br><br>
          Here are several examples of two visual recommendations. In each recommendation, the user input image is listed on the left and the top 9 most similar images from the dataset
          are displayed next to it. Also find a t-SNE plot of the corresponding images below. <br><br>
          <!-- T-SNE Embedding GIF -->
      <div style="text-align: center; width:100%; height: 600px; position: relative;">
        <figure class="figure" style="position: absolute; top:0; left: -100px; width: 50%">
          <img style="height: 600px;" alt="" src="example_hoodies.png">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion
            </span>
          </figcaption>
        </figure>
        <figure class="figure" style="position: absolute; top:0; right: -100px; width: 50%">
          <img style="height: 600px;" alt="" src="lakers.JPG">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion
            </span>
          </figcaption>
        </figure>
      </div>
      <br><br><br>
      <div style="text-align: center; width:100%; height: 400px; position: relative;">
        <figure class="figure" style="position: absolute; top:-40px; left: -100px; width: 50%">
          <img alt="" src="hoodie-tsne.JPG">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t-SNE Hoodie Projections
            </span>
          </figcaption>
        </figure>
        <figure class="figure" style="position: absolute; top:0; right: -100px; width: 50%">
          <img alt="" src="lakers-tsne.JPG">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t-SNE Jersey Projections
            </span>
          </figcaption>
        </figure>
      </div>
      <br><br><br>

      Here is an example of a failure case. Notice that the application is not well designed for images with significant background presence.

      <div style="text-align: center; margin-top: 20px;">
        <figure class="figure">
          <img style="height: 600px;" alt="" src="fail_case.png">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              Example of a Failure Case
            </span>
          </figcaption>
        </figure>
      </div> <br><br>
      Here is an example of the visual reccomendation using the final ResNet18 model with transfer learning compared to a visual
      reccomendation using the base pre-trained ResNet18 Model.<br><br>
          <!-- T-SNE Embedding GIF -->
      <div style="text-align: center; width:100%; height: 600px; position: relative;">
        <figure class="figure" style="position: absolute; top:0; left: -100px; width: 50%">
          <img style="height: 600px;" alt="" src="example_untrained_abstract.png">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion before Transfer Learning
            </span>
          </figcaption>
        </figure>
        <figure class="figure" style="position: absolute; top:0; right: -100px; width: 50%">
          <img style="height: 600px;" alt="" src="example_trained_abstract.png">
          <figcaption class="figure-caption">
            <span style="font-size: 12px; line-height: 1.5em;">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visual Recommendations using DeepFashion after Transfer Learning
            </span>
          </figcaption>
        </figure>
      </div>
      <br><br><br>
      <!-- Results -->
      <h3>Conclusion and Future Work</h3>
      In this project we’ve successfully implemented multiple neural networks to determine the image
      similarity between different categories of images in a fashion dataset. We obtained accuracies
      of ~95% and ~82% on the Fashion MNIST dataset with the Transfer Learning and Siamese Networks
      respectively. Subsequently, we’ve developed a visual search based recommender system using the
      Transfer Learning model on the more extensive ‘DeepFashion’ dataset and achieved an accuracy ~88% (top_5_accuracy).
      <br><br>
      The future scope of this project includes deploying the machine learning model online using an
      application like <a href="https://anvil.works/learn/tutorials/google-colab-to-web-app"
      target="_blank">Anvil</a>
      It is an interactive tool that allows connecting a web application to a colab notebook.
      The web application will retrieve a user query in real-time and send the data to the colab notebook.
      where the top-n similar items from the DeepFashion database will be computed using the Transfer Learning model
      and sent back to be displayed on the web application.
      <br><br>

      <!-- Results -->
      <h3>References</h3>
      Fashion MNIST
      <br>
      <a href="https://www.kaggle.com/zalando-research/fashionmnist">https://www.kaggle.com/zalando-research/fashionmnist</a>

      <br><br>
      A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning | by Dipanjan (DJ) Sarkar | Towards Data Science
      <br>
      <a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">
               https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a</a>
      <br><br>
      Transfer Learning with Convolutional Neural Networks in PyTorch | by Will Koehrsen | Towards Data Science
      <br>
      <a href="https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce">
               https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce</a>

      <br><br>
      Embeddings Visualization with Fashion MNIST
      <br>
      <a href="https://github.com/markjay4k/Fashion-MNIST-with-Keras/blob/master/pt3%20-%20FMINST%20Embeddings.ipynb">
        https://github.com/markjay4k/Fashion-MNIST-with-Keras/blob/master/pt3%20-%20FMINST%20Embeddings.ipynb</a>

      <br><br>
      Visualizing Data using the Embedding Projector in TensorBoard
      <br>
      <a href="https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin">https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin</a>

      <br><br>
      Learning Fine-grained Image Similarity with Deep Ranking
      <br>
      <a href="https://arxiv.org/pdf/1404.4661.pdf">https://arxiv.org/pdf/1404.4661.pdf</a>

      <br><br>
      Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce
      <br>
      <a href="https://arxiv.org/pdf/1703.02344.pdf">https://arxiv.org/pdf/1703.02344.pdf</a>

      <br><br>
      Building a Personalized Real-Time Fashion Collection Recommender
      <br>
      <a href="https://towardsdatascience.com/building-a-personalized-real-time-fashion-collection-recommender-22dc90c150cb">https://towardsdatascience.com/building-a-personalized-real-time-fashion-collection-recommender-22dc90c150cb</a>


      <br><br>


      <hr>
      <footer>
        <p>© Arzoo Irshad, Noah Meine, Jared Veerhoff</p>
      </footer>
    </div>
  </div>

  <br><br>

</body>

</html>
